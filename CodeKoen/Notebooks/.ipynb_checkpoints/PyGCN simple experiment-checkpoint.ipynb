{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# IMPORT 2D NUMPY ARRAY (THAT IS AN ADJACENCY MATRIX)\n",
    "# adj = ..\n",
    "\n",
    "adj = adj + sp.eye(adj.shape[0])  # add self links for normalization purposes and convert to sparse array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "normadj = normalize(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import features\n",
    "# features = ..\n",
    "# determine gcn dim1, dim2\n",
    "# dim1 = ..\n",
    "# dim2 = ..\n",
    "\n",
    "gc1 = GraphConvolution(dim1, dim2)\n",
    "gc1(features, adj)  # Compute one GCN round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT WITH MULTIDIMENSIONAL ADJACENCY MATRIX WHERE DIMENSIONS ARE KEPT (USING EINSUM)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n = 2 # nodes\n",
    "c = 4 # channels/features per node\n",
    "d = 3 # dimensions of the graph\n",
    "f1 = 10 # filter per dimension layer 1\n",
    "f2 = 5 # filter per dimension layer 2\n",
    "\n",
    "a = torch.arange(n*n*d).reshape(n, n, d)\n",
    "x = torch.arange(n*c).reshape(n, c, 1).repeat(1, 1, d)\n",
    "w1 = torch.arange(c*f1*d).reshape(c, f1, d)\n",
    "b1 = torch.arange(f1*d).reshape(1, f1, d)\n",
    "adj_x  = torch.einsum('ijd,jkd->ikd', a, x)\n",
    "out_1 = torch.einsum('ijd,jfd->ifd', adj_x, w1)\n",
    "out_bias_1 = out_1+b1\n",
    "activation = F.relu(out_bias_1)\n",
    "\n",
    "def print_m_per_d(name, m):\n",
    "    for i in range(m.shape[2]):\n",
    "        print(name, \"dimension\",i)\n",
    "        print(m[:,:,i])\n",
    "    print(\"\\n\")\n",
    "\n",
    "print_m_per_d(\"a\", a)\n",
    "print_m_per_d(\"x\", x)\n",
    "print_m_per_d(\"w1\", w1)\n",
    "print_m_per_d(\"b1\", b1)\n",
    "print_m_per_d(\"adj_x\", adj_x)\n",
    "print_m_per_d(\"out_1\", out_1)\n",
    "print_m_per_d(\"out_bias_1\", out_bias_1)\n",
    "print_m_per_d(\"activation\", activation)\n",
    "print(F.relu(out_bias_1[0,:,:]))\n",
    "print(w1)\n",
    "\n",
    "print(\"adj_x shape\", adj_x.shape)\n",
    "print(\"out_1 shape\", out_1.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
